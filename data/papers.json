{
  "total": 40,
  "papers": [
    {
      "venue": "ICLR",
      "year": 2025,
      "title": "基于激活梯度的后门攻击中毒样本检测",
      "title_en": "Activation Gradient based Poisoned Sample Detection Against Backdoor Attacks",
      "abstract": "这项工作研究了用于防御基于数据中毒的后门攻击的中毒样本检测任务。其核心挑战是找到一个可推广和有区别的指标来区分清洁和各种类型的中毒样本（例如，各种触发因素、各种中毒比率）。受后门攻击中一个常见现象的启发，即后门模型倾向于将目标类中明显不同的中毒和清洁样本映射到相似的激活区域，我们引入了一种新的视角，即梯度循环分布（GCD）。此外，我们基于GCD发现了两个有趣的观察结果。一个是目标类样本的GCD比清洁类样本分散得多。二是在目标类的GCD中，有毒和清洁样本被明确分开。受上述两个观察结果的启发，我们开发了一种创新的三阶段中毒样本检测方法，称为基于激活梯度的中毒样本检测（AGPD）。首先，我们根据在不可靠数据集上训练的模型计算所有类的GCD。然后，我们根据目标类和清洁类之间GCD离散度的差异来识别目标类。最后，我们根据中毒样本和清洁样本之间的明确分离，过滤出已识别目标类别内的中毒样本。在各种后门攻击设置下的广泛实验表明，根据基于样本激活的度量，所提出的方法比现有的中毒检测方法具有更优的检测性能。",
      "abstract_en": "This work studies the task of poisoned sample detection for defending against data poisoning based backdoor attacks. Its core challenge is finding a generalizable and discriminative metric to distinguish between clean and various types of poisoned samples (e.g., various triggers, various poisoning ratios). Inspired by a common phenomenon in backdoor attacks that the backdoored model tend to map significantly different poisoned and clean samples within the target class to similar activation areas, we introduce a novel perspective of the circular distribution of the gradients w.r.t. sample activation, dubbed gradient circular distribution (GCD). And, we find two interesting observations based on GCD. One is that the GCD of samples in the target class is much more dispersed than that in the clean class. The other is that in the GCD of target class, poisoned and clean samples are clearly separated. Inspired by above two observations, we develop an innovative three-stage poisoned sample detection approach, called Activation Gradient based Poisoned sample Detection (AGPD). First, we calculate GCDs of all classes from the model trained on the untrustworthy dataset. Then, we identify the target class(es) based on the difference on GCD dispersion between target and clean classes. Last, we filter out poisoned samples within the identified target class(es) based on the clear separation between poisoned and clean samples. Extensive experiments under various settings of backdoor attacks demonstrate the superior detection performance of the proposed method to existing poisoned detection approaches according to sample activation-based metrics.",
      "keywords": "Backdoor Defense; Poisoned Sample Detection; AI security",
      "authors": "Danni Yuan, Mingda Zhang, Shaokui Wei, Li Liu, Baoyuan Wu",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=VNMJfBBUd5",
      "openreview_forum_url": "https://openreview.net/forum?id=VNMJfBBUd5",
      "venue_id": "ICLR.cc/2025/Conference"
    },
    {
      "venue": "ICLR",
      "year": 2025,
      "title": "Agent定义：通过精化调优增强Agent泛化",
      "title_en": "AgentRefine: Enhancing Agent Generalization through Refinement Tuning",
      "abstract": "基于大型语言模型（LLM）的代理已经证明它们能够像人类一样执行复杂的任务。然而，开源LLM和GPT系列等商业模式之间仍然存在很大差距。本文主要研究通过指令调优来提高LLM的代理泛化能力。我们首先观察到，现有的代理训练语料库在保留评估集上表现出令人满意的结果，但未能推广到保留集。这些代理调优工作面临严重的格式错误，并且经常长时间陷入同一错误。我们分析，泛化能力差的原因是对多个手动代理环境的过度拟合以及对新情况的适应性不足。他们与错误的动作步骤作斗争，不能从经验中学习，只能记住现有的观察动作关系。受此启发，我们提出了一种新的代理调优框架AgentDefine。核心思想是使模型能够通过观察轨迹来学习纠正错误。具体来说，我们提出了一个代理综合框架，以涵盖各种环境和任务，并促使强大的LLM根据环境反馈来改进其错误操作。AgentRefine在各种代理任务的泛化能力方面明显优于最先进的代理调优工作。它在面对扰动时也具有更好的鲁棒性，可以在推理中产生多样化的思维。我们的研究结果建立了代理泛化和自我精炼之间的相关性，并为未来的研究提供了新的范式。",
      "abstract_en": "Large Language Model (LLM) based agents have proved their ability to perform complex tasks like humans. However, there is still a large gap between open-sourced LLMs and commercial models like the GPT series. In this paper, we focus on improving the agent generalization capabilities of LLMs via instruction tuning. We first observe that the existing agent training corpus exhibits satisfactory results on held-in evaluation sets but fails to generalize to held-out sets. These agent-tuning works face severe formatting errors and are frequently stuck in the same mistake for a long while. We analyze that the poor generalization ability comes from overfitting to several manual agent environments and a lack of adaptation to new situations. They struggle with the wrong action steps and can not learn from the experience but just memorize existing observation-action relations. Inspired by the insight, we propose a novel AgentRefine framework for agent-tuning. The core idea is to enable the model to learn to correct its mistakes via observation in the trajectory. Specifically, we propose an agent synthesis framework to encompass a diverse array of environments and tasks and prompt a strong LLM to refine its error action according to the environment feedback. AgentRefine significantly outperforms state-of-the-art agent-tuning work in terms of generalization ability on diverse agent tasks. It also has better robustness facing perturbation and can generate diversified thought in inference. Our findings establish the correlation between agent generalization and self-refinement and provide a new paradigm for future research.",
      "keywords": "agent; self-refine; diversity; generalization; data synthesis",
      "authors": "Dayuan Fu, Keqing He, Yejie Wang, Wentao Hong, Zhuoma GongQue, Weihao Zeng, Wei Wang, Jingang Wang, Xunliang Cai, Weiran Xu",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=FDimWzmcWn",
      "openreview_forum_url": "https://openreview.net/forum?id=FDimWzmcWn",
      "venue_id": "ICLR.cc/2025/Conference"
    },
    {
      "venue": "ICLR",
      "year": 2025,
      "title": "（亚）线性宽度神经网络中经验核谱的贝叶斯处理",
      "title_en": "Bayesian Treatment of the Spectrum of the Empirical Kernel in (Sub)Linear-Width Neural Networks",
      "abstract": "我们在训练样本数量、网络宽度和输入空间维数无限增加的理论限制下研究贝叶斯神经网络（BNNs）。我们的发现通过Mercer特征值与随机矩阵理论中研究的协方差矩阵的极限谱分布之间的对应关系，在核理论方法和统计力学技术之间建立了新的桥梁。 ",
      "abstract_en": "We study Bayesian neural networks (BNNs) in the theoretical limits of infinitely increasing number of training examples, network width and input space dimension. Our findings establish new bridges between kernel-theoretic approaches and techniques derived from statistical mechanics through the correspondence between Mercer's eigenvalues and limiting spectral distributions of covariance matrices studied in random matrix theory. \n   Our theoretical contributions first consist in novel integral formulas that accurately describe the predictors of BNNs in the asymptotic linear-width and sublinear-width regimes. Moreover, we extend the recently developed renormalisation theory of deep linear neural networks, enabling a rigorous explanation of the mounting empirical evidence that hints at the theory's applicability to nonlinear BNNs with ReLU activations in the linear-width regime.\n   From a practical standpoint, our results introduce a novel technique for estimating the predictor statistics of a trained BNN that is applicable to the sublinear-width regime where the predictions of the renormalisation theory are inaccurate.",
      "keywords": "infinite bayesian neural networks; kernel theory; random matrix theory",
      "authors": "Ouns El Harzli, Bernardo Cuenca Grau",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=O6znYvxC1U",
      "openreview_forum_url": "https://openreview.net/forum?id=O6znYvxC1U",
      "venue_id": "ICLR.cc/2025/Conference"
    },
    {
      "venue": "ICLR",
      "year": 2025,
      "title": "超越随机掩蔽：当Dropout遇到图卷积网络时",
      "title_en": "Beyond Random Masking: When Dropout meets Graph Convolutional Networks",
      "abstract": "图卷积网络（GCN）已经成为学习图结构数据的强大工具，但这些模型中的辍学行为仍然知之甚少。本文对GCN中的dropout进行了全面的理论分析，揭示了它的主要作用与标准神经网络有着根本的不同——防止过度平滑，而不是共同适应。我们证明了GCN中的dropout会创建特定维度的随机子图，从而导致一种标准神经网络中不存在的结构正则化形式。我们的分析表明，丢包效应本质上是程度相关的，从而产生了考虑节点拓扑重要性的自适应正则化。我们提供了关于dropout在缓解过度光滑中的作用的新见解，并推导出了新的泛化界限，解释了图特定的dropout效应。此外，我们分析了GCN中dropout和批归一化之间的协同作用，揭示了一种增强整体正则化的机制。我们的理论发现通过在14个数据集的节点级和图级任务上进行的广泛实验得到了验证。值得注意的是，具有dropout和批归一化的GCN在几个基准测试中表现优于最先进的方法，这证明了我们理论见解的实际影响。",
      "abstract_en": "Graph Convolutional Networks (GCNs) have emerged as powerful tools for learning on graph-structured data, yet the behavior of dropout in these models remains poorly understood. This paper presents a comprehensive theoretical analysis of dropout in GCNs, revealing that its primary role differs fundamentally from standard neural networks - preventing oversmoothing rather than co-adaptation. We demonstrate that dropout in GCNs creates dimension-specific stochastic sub-graphs, leading to a form of structural regularization not present in standard neural networks. Our analysis shows that dropout effects are inherently degree-dependent, resulting in adaptive regularization that considers the topological importance of nodes. We provide new insights into dropout's role in mitigating oversmoothing and derive novel generalization bounds that account for graph-specific dropout effects. Furthermore, we analyze the synergistic interaction between dropout and batch normalization in GCNs, uncovering a mechanism that enhances overall regularization. Our theoretical findings are validated through extensive experiments on both node-level and graph-level tasks across 14 datasets. Notably, GCN with dropout and batch normalization outperforms state-of-the-art methods on several benchmarks, demonstrating the practical impact of our theoretical insights.",
      "keywords": "Graph neural networks; Dropout",
      "authors": "Yuankai Luo, Xiao-Ming Wu, Hao Zhu",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=PwxYoMvmvy",
      "openreview_forum_url": "https://openreview.net/forum?id=PwxYoMvmvy",
      "venue_id": "ICLR.cc/2025/Conference"
    },
    {
      "venue": "ICLR",
      "year": 2025,
      "title": "大型语言模型的因果激励同步抑制",
      "title_en": "Causally Motivated Sycophancy Mitigation for Large Language Models",
      "abstract": "将用户偏好纳入大型语言模型（LLM）可以增强模型输出的个性化和可靠性，并促进LLM在现实世界场景中的应用。然而，利用用户偏好可能是一把双刃剑。最近的研究发现，不当使用可能会导致阿谀奉承，LLM优先考虑与用户偏好的一致性，而不是其输出的正确性。为了解决LLM中的阿谀奉承问题，我们通过结构化因果模型（SCM）的视角对问题进行分析和建模。本文将阿谀奉承归因于LLM对用户偏好和模型输出之间虚假相关性的依赖。基于所提出的SCM，我们开发了一个新的框架，称为**CAUSM**，通过利用重要的因果特征来减轻LLM中的阿谀奉承。具体来说，我们通过因果激励的头部重新加权来消除LLM中间层中嵌入的虚假相关性，然后沿着因果表示方向校准头部内部知识。在不同的语言任务中进行了广泛的实验，以证明我们的方法在减轻LLM中的阿谀奉承方面优于最先进的竞争对手。",
      "abstract_en": "Incorporating user preferences into large language models (LLMs) can enhance the personalization and reliability of model outputs and facilitate the application of LLMs to real-world scenarios. However, leveraging user preferences can be a double-edged sword. Recent studies have found that improper utilization can incur sycophancy, where LLMs prioritize alignment with user preferences over the correctness of their outputs. To address sycophancy in LLMs, we analyze and model the problem through the lens of structured causal models (SCMs). We attribute sycophancy to LLMs' reliance on spurious correlations between user preferences and model outputs in this paper. Based on the proposed SCMs, we develop a novel framework, termed **CAUSM**, to mitigate sycophancy in LLMs by exploiting a significant causal signature. Specifically, we eliminate the spurious correlations embedded in the intermediate layers of LLMs through causally motivated head reweighting, and then calibrate the intra-head knowledge along the causal representation direction. Extensive experiments are conducted across diverse language tasks to demonstrate the superiority of our method over state-of-the-art competitors in mitigating sycophancy in LLMs.",
      "keywords": "Large Language Model; Sycophancy; Causal Modeling",
      "authors": "Haoxi Li, Xueyang Tang, Jie ZHANG, Song Guo, Sikai Bai, Peiran Dong, Yue Yu",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=yRKelogz5i",
      "openreview_forum_url": "https://openreview.net/forum?id=yRKelogz5i",
      "venue_id": "ICLR.cc/2025/Conference"
    },
    {
      "venue": "ICLR",
      "year": 2025,
      "title": "基于组合仿真的时间序列推理",
      "title_en": "Compositional simulation-based inference for time series",
      "abstract": "Amortized simulation-based inference (SBI) methods train neural networks on simulated data to perform Bayesian inference. While this strategy avoids the need for tractable likelihoods, it often requires a large number of simulations and has been challenging to scale to time series data. Scientific simulators frequently emulate real-world dynamics through thousands of single-state transitions over time. We propose an SBI approach that can exploit such Markovian simulators by locally identifying parameters consistent with individual state transitions. We then compose these local results to obtain a posterior over parameters that align with the entire time series observation. We focus on applying this approach to neural posterior score estimation but also show how it can be applied, e.g., to neural likelihood (ratio) estimation. We demonstrate that our approach is more simulation-efficient than directly estimating the global posterior on several synthetic benchmark tasks and simulators used in ecology and epidemiology. Finally, we validate scalability and simulation efficiency of our approach by applying it to a high-dimensional Kolmogorov flow simulator with around one million data dimensions.",
      "abstract_en": "Amortized simulation-based inference (SBI) methods train neural networks on simulated data to perform Bayesian inference. While this strategy avoids the need for tractable likelihoods, it often requires a large number of simulations and has been challenging to scale to time series data. Scientific simulators frequently emulate real-world dynamics through thousands of single-state transitions over time. We propose an SBI approach that can exploit such Markovian simulators by locally identifying parameters consistent with individual state transitions. We then compose these local results to obtain a posterior over parameters that align with the entire time series observation. We focus on applying this approach to neural posterior score estimation but also show how it can be applied, e.g., to neural likelihood (ratio) estimation. We demonstrate that our approach is more simulation-efficient than directly estimating the global posterior on several synthetic benchmark tasks and simulators used in ecology and epidemiology. Finally, we validate scalability and simulation efficiency of our approach by applying it to a high-dimensional Kolmogorov flow simulator with around one million data dimensions.",
      "keywords": "Simulation-based inference; Bayesian inference; time series; markovian simulators; Amortized Bayesian inference",
      "authors": "Manuel Gloeckler, Shoji Toyota, Kenji Fukumizu, Jakob H. Macke",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=uClUUJk05H",
      "openreview_forum_url": "https://openreview.net/forum?id=uClUUJk05H",
      "venue_id": "ICLR.cc/2025/Conference"
    },
    {
      "venue": "ICLR",
      "year": 2025,
      "title": "DarkBench：大型语言模型中的暗模式基准测试",
      "title_en": "DarkBench: Benchmarking Dark Patterns in Large Language Models",
      "abstract": "我们介绍了DarkBench，这是一个全面的基准，用于检测与大型语言模型（LLM）交互中的暗设计模式——影响用户行为的操纵技术。我们的基准包括六个类别的660个提示：品牌偏见、用户保留、阿谀奉承、拟人化、有害生成和偷偷摸摸。我们评估了五家领先公司（OpenAI、Anthropic、Meta、Mistral、Google）的模型，发现一些LLM的设计明显偏向于开发人员的产品，并表现出不真实的沟通以及其他操纵行为。开发LLM的公司应该认识到并减轻黑暗设计模式的影响，以促进更道德的人工智能。",
      "abstract_en": "We introduce DarkBench, a comprehensive benchmark for detecting dark design patterns—manipulative techniques that influence user behavior—in interactions with large language models (LLMs). Our benchmark comprises 660 prompts across six categories: brand bias, user retention, sycophancy, anthropomorphism, harmful generation, and sneaking. We evaluate models from five leading companies (OpenAI, Anthropic, Meta, Mistral, Google) and find that some LLMs are explicitly designed to favor their developers' products and exhibit untruthful communication, among other manipulative behaviors. Companies developing LLMs should recognize and mitigate the impact of dark design patterns to promote more ethical Al.",
      "keywords": "Dark Patterns; AI Deception; Large Language Models",
      "authors": "Esben Kran, Hieu Minh Nguyen, Akash Kundu, Sami Jawhar, Jinsuk Park, Mateusz Maria Jurewicz",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=odjMSBSWRt",
      "openreview_forum_url": "https://openreview.net/forum?id=odjMSBSWRt",
      "venue_id": "ICLR.cc/2025/Conference"
    },
    {
      "venue": "ICLR",
      "year": 2025,
      "title": "GANDALF：基于生成注意力的数据增强和预测模式用于个性化癌症治疗的Ling框架",
      "title_en": "GANDALF: Generative AttentioN based Data Augmentation and predictive modeLing Framework for personalized cancer treatment",
      "abstract": "癌症的有效治疗是医疗保健提供者面临的一个主要挑战，因为患者对治疗的反应具有高度个性化的性质。这是由患者基因组中导致癌症改变（突变）的异质性引起的。患者反应数据的有限可用性使得难以根据临床基因组测序报告中的突变训练个性化治疗推荐模型。先前的方法通过迁移学习利用更大的、标记的临床前实验室数据集（“细胞系”）来解决这个问题。这些方法通过学习细胞系和患者域之间的共享域不变表示来增强患者数据，然后将其用于训练下游药物反应预测（DRP）模型。这种方法增强了共享空间中的数据，但无法模拟患者的特定特征，这些特征对他们的药物反应有很强的影响。我们提出了一种新的基于生成注意力的数据增强和预测建模框架GANDALF，以解决现有方法的这一关键缺点。GANDALF不仅可以直接增强患者基因组数据，还可以解释其领域特异性特征。在公开的患者数据集上，GANDALF优于最先进的DRP模型，并成为SOTA癌症DRP模型的前沿。",
      "abstract_en": "Effective treatment of cancer is a major challenge faced by healthcare providers, due to the highly individualized nature of patient responses to treatment. This is caused by the heterogeneity seen in cancer-causing alterations (mutations) across patient genomes. Limited availability of response data in patients makes it difficult to train personalized treatment recommendation models on mutations from clinical genomic sequencing reports. Prior methods tackle this by utilising larger, labelled pre-clinical laboratory datasets (‘cell lines’), via transfer learning. These methods augment patient data by learning a shared, domain-invariant representation, between the cell line and patient domains, which is then used to train a downstream drug response prediction (DRP) model. This approach augments data in the shared space but fails to model patient-specific characteristics, which have a strong influence on their drug response. We propose a novel generative attention-based data augmentation and predictive modeling framework, GANDALF, to tackle this crucial shortcoming of prior methods. GANDALF not only augments patient genomic data directly, but also accounts for its domain-specific characteristics. GANDALF outperforms state-of-the-art DRP models on publicly available patient datasets and emerges as the front-runner amongst SOTA cancer DRP models.",
      "keywords": "personalized drug response prediction; cancer; genomic data augmentation; diffusion model; pseudolabelling",
      "authors": "Aishwarya Jayagopal, Yanrong Zhang, Robert John Walsh, Tuan Zea Tan, Anand D Jeyasekharan, Vaibhav Rajan",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=WwmtcGr4lP",
      "openreview_forum_url": "https://openreview.net/forum?id=WwmtcGr4lP",
      "venue_id": "ICLR.cc/2025/Conference"
    },
    {
      "venue": "ICLR",
      "year": 2025,
      "title": "基于边界熵最小化的多标签测试时间自适应",
      "title_en": "Multi-Label Test-Time Adaptation with Bound Entropy Minimization",
      "abstract": "主流测试时间自适应（TTA）技术试图通过熵最小化来减轻多类分类的分布变化，从而固有地提高了最自信类的概率。然而，当遇到多标签实例时，主要的挑战源于每张图像的标签数量不同，只优先考虑概率最高的类别不可避免地会破坏其他正面标签的适应性。为了解决这个问题，我们研究了多标签场景中的TTA（ML-TTA），开发了边界熵最小化（BEM）目标，以同时提高多个顶部预测标签的置信度。具体来说，为了确定每个增强视图的标签数量，我们检索一个成对的标题，并为该视图生成文本标签。这些标签被分配给视图和字幕，称为弱标签集和强标签集，大小相同。在此之后，所提出的边界元法将视图和字幕中预测的最高前k个标签分别视为一个实体，同时学习视图和字幕提示。通过绑定top-k预测标签，边界元法克服了香草熵最小化的局限性，香草熵最小化只优化了最有信心的类。在MSCOCO、VOC和NUSWIDE多标签数据集中，与最新的SOTA方法相比，我们配备BEM的ML-TTA框架在各种模型架构、快速初始化和不同的标签场景中表现出卓越的性能。该代码可在以下网址获得https://github.com/Jinx630/ML-TTA.",
      "abstract_en": "Mainstream test-time adaptation (TTA) techniques endeavor to mitigate distribution shifts via entropy minimization for multi-class classification, inherently increasing the probability of the most confident class. However, when encountering multi-label instances, the primary challenge stems from the varying number of labels per image, and prioritizing only the highest probability class inevitably undermines the adaptation of other positive labels. To address this issue, we investigate TTA within multi-label scenario (ML--TTA), developing Bound Entropy Minimization (BEM) objective to simultaneously increase the confidence of multiple top predicted labels. Specifically, to determine the number of labels for each augmented view, we retrieve a paired caption with yielded textual labels for that view. These labels are allocated to both the view and caption, called weak label set and strong label set with the same size k. Following this, the proposed BEM considers the highest top-k predicted labels from view and caption as a single entity, respectively, learning both view and caption prompts concurrently. By binding top-k predicted labels, BEM overcomes the limitation of vanilla entropy minimization, which exclusively optimizes the most confident class. Across the MSCOCO, VOC, and NUSWIDE multi-label datasets, our ML--TTA framework equipped with BEM exhibits superior performance compared to the latest SOTA methods, across various model architectures, prompt initialization, and varying label scenarios. The code is available at https://github.com/Jinx630/ML-TTA.",
      "keywords": "Vision-Language Models; Zero-Shot Multi-Label Generalization; Test-Time Adaptation",
      "authors": "Xiangyu Wu, Feng Yu, Yang Yang, Qing-Guo Chen, Jianfeng Lu",
      "github_links": "https://github.com/Jinx630/ML-TTA",
      "pdf_download_url": "https://openreview.net/pdf?id=75PhjtbBdr",
      "openreview_forum_url": "https://openreview.net/forum?id=75PhjtbBdr",
      "venue_id": "ICLR.cc/2025/Conference"
    },
    {
      "venue": "ICLR",
      "year": 2025,
      "title": "基于离线模型的学习排序优化",
      "title_en": "Offline Model-Based Optimization by Learning to Rank",
      "abstract": "离线模型优化（MBO）旨在识别一种设计，该设计仅使用固定的、预先收集的设计数据集及其相应的分数来最大化黑盒函数。这个问题引起了科学和工业领域的广泛关注。离线MBO中的一种常见方法是通过最小化均方误差（MSE）来训练基于回归的代理模型，然后通过不同的优化器（例如梯度上升）在该代理模型中找到最佳设计。然而，一个关键的挑战是分布外误差的风险，即替代模型通常会高估分数，并将优化器误导到次优区域。先前的工作试图以各种方式解决这个问题，例如使用正则化技术和集成学习来增强模型的鲁棒性，但它仍然存在。在这篇论文中，我们认为，用MSE训练的回归模型与离线MBO的主要目标并不一致，离线MBO主要目标是选择有前景的设计，而不是精确预测它们的得分。值得注意的是，如果代理模型可以根据候选设计的相对得分关系来维持候选设计的顺序，即使没有精确的预测，它也可以产生最佳设计。为了验证这一点，我们进行了实验来比较最终设计的质量与MSE之间的关系，发现相关性非常弱。相比之下，衡量订单维护质量的指标显示出明显更强的相关性。基于这一观察，我们建议学习一种基于排名的模型，该模型利用学习排名技术，根据相对得分对有前景的设计进行优先级排序。我们证明了排名损失的泛化误差可以很好地限制。跨不同任务的实证结果表明，我们提出的基于排名的方法比现有的二十种方法具有更优的性能。我们的实现可以在\\url上找到{https://github.com/lamda-bbo/Offline-RaM}.",
      "abstract_en": "Offline model-based optimization (MBO) aims to identify a design that maximizes a black-box function using only a fixed, pre-collected dataset of designs and their corresponding scores. This problem has garnered significant attention from both scientific and industrial domains. A common approach in offline MBO is to train a regression-based surrogate model by minimizing mean squared error (MSE) and then find the best design within this surrogate model by different optimizers (e.g., gradient ascent). However, a critical challenge is the risk of out-of-distribution errors, i.e., the surrogate model may typically overestimate the scores and mislead the optimizers into suboptimal regions. Prior works have attempted to address this issue in various ways, such as using regularization techniques and ensemble learning to enhance the robustness of the model, but it still remains. In this paper, we argue that regression models trained with MSE are not well-aligned with the primary goal of offline MBO, which is to \\textit{select} promising designs rather than to predict their scores precisely. Notably, if a surrogate model can maintain the order of candidate designs based on their relative score relationships, it can produce the best designs even without precise predictions. To validate it, we conduct experiments to compare the relationship between the quality of the final designs and MSE, finding that the correlation is really very weak. In contrast, a metric that measures order-maintaining quality shows a significantly stronger correlation. Based on this observation, we propose learning a ranking-based model that leverages learning to rank techniques to prioritize promising designs based on their relative scores. We show that the generalization error on ranking loss can be well bounded. Empirical results across diverse tasks demonstrate the superior performance of our proposed ranking-based method than twenty existing methods. Our implementation is available at \\url{https://github.com/lamda-bbo/Offline-RaM}.",
      "keywords": "Offline model-based optimization; black-box optimization; learning to rank; learning to optimize",
      "authors": "Rong-Xi Tan, Ke Xue, Shen-Huan Lyu, Haopu Shang, yaowang, Yaoyuan Wang, Fu Sheng, Chao Qian",
      "github_links": "https://github.com/lamda-bbo/Offline-RaM",
      "pdf_download_url": "https://openreview.net/pdf?id=sb1HgVDLjN",
      "openreview_forum_url": "https://openreview.net/forum?id=sb1HgVDLjN",
      "venue_id": "ICLR.cc/2025/Conference"
    },
    {
      "venue": "ICLR",
      "year": 2025,
      "title": "时间序列输入的最优传输",
      "title_en": "Optimal Transport for Time Series Imputation",
      "abstract": "通过分布对齐进行缺失数据插补已经证明了非时间数据集的优势，但在时间序列应用中表现不佳。主要障碍是制定一个差异度量，同时（1）捕捉时间模式——考虑时间序列中固有的周期性和时间依赖性——和（2）适应非平稳性，确保在多种共存的时间模式中的稳健性。为了应对这些挑战，我们引入了近谱Wasserstein（PSW）差异，这是一种新的差异，专门用于比较基于最优传输的两组时间序列。它结合了成对光谱距离来封装时间模式，以及选择性匹配正则化来适应非平稳性。随后，我们开发了PSW for Imputation（PSW-I）框架，该框架通过最小化PSW差异来迭代地改进插补结果。大量实验表明，PSW-I有效地适应了时间模式和非平稳性，优于当前流行的时间序列插补方法。代码可在以下网址获得https://github.com/FMLYD/PSW-I.",
      "abstract_en": "Missing data imputation through distribution alignment has demonstrated advantages for non-temporal datasets but exhibits suboptimal performance in time-series applications. The primary obstacle is crafting a discrepancy measure that simultaneously (1) captures temporal patterns—accounting for periodicity and temporal dependencies inherent in time-series—and (2) accommodates non-stationarity, ensuring robustness amidst multiple coexisting temporal patterns. In response to these challenges, we introduce the Proximal Spectrum Wasserstein (PSW) discrepancy, a novel discrepancy tailored for comparing two \\textit{sets} of time-series based on optimal transport. It incorporates a pairwise spectral distance to encapsulate temporal patterns, and a selective matching regularization to accommodate non-stationarity. Subsequently, we develop the PSW for Imputation (PSW-I) framework, which iteratively refines imputation results by minimizing the PSW discrepancy. Extensive experiments demonstrate that PSW-I effectively accommodates temporal patterns and non-stationarity, outperforming prevailing time-series imputation methods. Code is available at https://github.com/FMLYD/PSW-I.",
      "keywords": "Time series; Imputation",
      "authors": "Hao Wang, zhengnan li, Haoxuan Li, Xu Chen, Mingming Gong, BinChen, Zhichao Chen",
      "github_links": "https://github.com/FMLYD/PSW-I",
      "pdf_download_url": "https://openreview.net/pdf?id=xPTzjpIQNp",
      "openreview_forum_url": "https://openreview.net/forum?id=xPTzjpIQNp",
      "venue_id": "ICLR.cc/2025/Conference"
    },
    {
      "venue": "ICLR",
      "year": 2025,
      "title": "PIED：逆问题的物理知情实验设计",
      "title_en": "PIED: Physics-Informed Experimental Design for Inverse Problems",
      "abstract": "在许多科学和工程环境中，系统动力学的特征是控制偏微分方程（PDE），一个主要的挑战是解决逆问题（IP），其中未知的PDE参数是根据在有限预算下收集的观测数据推断出来的。 ",
      "abstract_en": "In many science and engineering settings, system dynamics are characterized by governing partial differential equations (PDEs), and a major challenge is to solve inverse problems (IPs) where unknown PDE parameters are inferred based on observational data gathered under limited budget. \nDue to the high costs of setting up and running experiments, experimental design (ED) is often done with the help of PDE simulations to optimize for the most informative design parameters (e.g., sensor placements) to solve such IPs, prior to actual data collection. This process of optimizing design parameters is especially critical when the budget and other practical constraints make it infeasible to adjust the design parameters between trials during the experiments.\nHowever, existing experimental design (ED) methods tend to require sequential and frequent design parameter adjustments between trials. Furthermore, they also have significant computational bottlenecks due to the need for complex numerical simulations for PDEs, and do not exploit the advantages provided by physics informed neural networks (PINNs) in solving IPs for PDE-governed systems, such as its meshless solutions, differentiability, and amortized training. \nThis work presents Physics-Informed Experimental Design (PIED), the first ED framework that makes use of PINNs in a fully differentiable architecture to perform continuous optimization of design parameters for IPs for one-shot deployments. \nPIED overcomes existing methods' computational bottlenecks through parallelized computation and meta-learning of PINN parameter initialization, and proposes novel methods to effectively take into account PINN training dynamics in optimizing the ED parameters. \nThrough experiments based on noisy simulated data and even real world experimental data, we empirically show that given limited observation budget, PIED significantly outperforms existing ED methods in solving IPs, including for challenging settings where the inverse parameters are unknown functions rather than just finite-dimensional.",
      "keywords": "Physics-Informed Neural Network; PINNs; Experimental Design; AI For Science; Active Learning; Data Selection",
      "authors": "Apivich Hemachandra, Gregory Kang Ruey Lau, See-Kiong Ng, Bryan Kian Hsiang Low",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=w7P92BEsb2",
      "openreview_forum_url": "https://openreview.net/forum?id=w7P92BEsb2",
      "venue_id": "ICLR.cc/2025/Conference"
    },
    {
      "venue": "ICLR",
      "year": 2025,
      "title": "RaSA：等级共享低等级适应",
      "title_en": "RaSA: Rank-Sharing Low-Rank Adaptation",
      "abstract": "低秩自适应（LoRA）已被广泛用于大型语言模型（LLM）的参数高效微调。然而，由于低秩约束，LoRA的有限表达能力已被公认为是一个瓶颈，特别是在代码生成和数学推理等严格任务中。为了解决这一局限性，我们引入了秩共享低秩自适应（RaSA），这是一种创新的扩展，通过利用跨层的部分秩共享来增强LoRA的表达能力。通过形成共享排名池并应用特定于层的加权，RaSA有效地增加了排名数量，而不会增加参数开销。我们的理论基础和实证验证方法表明，RaSA不仅保持了LoRA的核心优势，而且显著提高了具有挑战性的代码和数学任务的性能。代码、数据和脚本可在以下网址获得：https://github.com/zwhe99/RaSA.",
      "abstract_en": "Low-rank adaptation (LoRA) has been prominently employed for parameter-efficient fine-tuning of large language models (LLMs). However, the limited expressive capacity of LoRA, stemming from the low-rank constraint, has been recognized as a bottleneck, particularly in rigorous tasks like code generation and mathematical reasoning. To address this limitation, we introduce Rank-Sharing Low-Rank Adaptation (RaSA), an innovative extension that enhances the expressive capacity of LoRA by leveraging partial rank sharing across layers. By forming a shared rank pool and applying layer-specific weighting, RaSA effectively increases the number of ranks without augmenting parameter overhead. Our theoretically grounded and empirically validated approach demonstrates that RaSA not only maintains the core advantages of LoRA but also significantly boosts performance in challenging code and math tasks. Code, data and scripts are available at: https://github.com/zwhe99/RaSA.",
      "keywords": "parameter-efficient fine-tuning; large language model; low-rank adaptation",
      "authors": "Zhiwei He, Zhaopeng Tu, Xing Wang, Xingyu Chen, Zhijie Wang, Jiahao Xu, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Rui Wang",
      "github_links": "https://github.com/zwhe99/RaSA",
      "pdf_download_url": "https://openreview.net/pdf?id=GdXI5zCoAt",
      "openreview_forum_url": "https://openreview.net/forum?id=GdXI5zCoAt",
      "venue_id": "ICLR.cc/2025/Conference"
    },
    {
      "venue": "ICLR",
      "year": 2025,
      "title": "用合成交织数据缩放语音文本预训练",
      "title_en": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data",
      "abstract": "语音语言模型（SpeechLM）接受语音输入并产生语音输出，与基于文本的大型语言模型（LLM）相比，允许更自然的人机交互。",
      "abstract_en": "Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs).\nTraditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant compared to text pre-training data, thereby limiting their scalability as LLMs.\nWe propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets.\nOur method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech.\nWe also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model  by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower sampling rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality.\nStarting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in both speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13\\% (Moshi) to 31\\%.\nWe further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.",
      "keywords": "large language models; speech language model; spoken chatbots",
      "authors": "Aohan Zeng, Zhengxiao Du, Mingdao Liu, Lei Zhang, shengmin jiang, Yuxiao Dong, Jie Tang",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=3tukjsVyrE",
      "openreview_forum_url": "https://openreview.net/forum?id=3tukjsVyrE",
      "venue_id": "ICLR.cc/2025/Conference"
    },
    {
      "venue": "ICLR",
      "year": 2025,
      "title": "自监督对比学习执行非线性系统识别",
      "title_en": "Self-supervised contrastive learning performs non-linear system identification",
      "abstract": "自监督学习（SSL）方法在许多任务和领域取得了巨大的成功。有人认为，这些成功可以归因于SSL和可识别表示学习之间的联系：时间结构和辅助变量确保潜在表示与数据的真实潜在生成因素相关。在这里，我们深化了这种联系，并表明SSL可以在潜在空间中执行系统识别。我们提出了动力学对比学习，这是一个在非线性观测模型下揭示线性、切换线性和非线性动力学的框架，给出了理论保证并进行了实证验证。",
      "abstract_en": "Self-supervised learning (SSL) approaches have brought tremendous success across many tasks and domains. It has been argued that these successes can be attributed to a link between SSL and identifiable representation learning: Temporal structure and auxiliary variables ensure that latent representations are related to the true underlying generative factors of the data. Here, we deepen this connection and show that SSL can perform system identification in latent space. We propose dynamics contrastive learning, a framework to uncover linear, switching linear and non-linear dynamics under a non-linear observation model, give theoretical guarantees and validate them empirically.",
      "keywords": "system identification; dynamics learning; identifiability; self-supervised learning",
      "authors": "Rodrigo González Laiz, Tobias Schmidt, Steffen Schneider",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=ONfWFluZBI",
      "openreview_forum_url": "https://openreview.net/forum?id=ONfWFluZBI",
      "venue_id": "ICLR.cc/2025/Conference"
    },
    {
      "venue": "ICLR",
      "year": 2025,
      "title": "稀疏的自动编码器揭示了自适应过程中视觉概念的选择性重映射",
      "title_en": "Sparse autoencoders reveal selective remapping of visual concepts during adaptation",
      "abstract": "为特定目的调整基础模型已成为为下游应用构建机器学习系统的标准方法。然而，在适应过程中发生了哪些机制，这是一个悬而未决的问题。在这里，我们为CLIP视觉变换器开发了一种新的稀疏自编码器（SAE），名为PatchSAE，用于提取粒度级别的可解释概念（例如，对象的形状、颜色或语义）及其逐块空间属性。我们探讨了这些概念如何影响下游图像分类任务中的模型输出，并研究了最新的基于提示的自适应技术如何改变模型输入与这些概念的关联。虽然适应和非适应模型之间概念的激活略有变化，但我们发现，常见适应任务的大部分收益可以用非适应基础模型中已经存在的现有概念来解释。这项工作为训练和使用视觉转换器的SAE提供了一个具体的框架，并为解释适应机制提供了见解。",
      "abstract_en": "Adapting foundation models for specific purposes has become a standard approach to build machine learning systems for downstream applications. Yet, it is an open question which mechanisms take place during adaptation. Here we develop a new Sparse Autoencoder (SAE) for the CLIP vision transformer, named PatchSAE, to extract interpretable concepts at granular levels (e.g., shape, color, or semantics of an object) and their patch-wise spatial attributions. We explore how these concepts influence the model output in downstream image classification tasks and investigate how recent state-of-the-art prompt-based adaptation techniques change the association of model inputs to these concepts. While activations of concepts slightly change between adapted and non-adapted models, we find that the majority of gains on common adaptation tasks can be explained with the existing concepts already present in the non-adapted foundation model. This work provides a concrete framework to train and use SAEs for Vision Transformers and provides insights into explaining adaptation mechanisms.",
      "keywords": "interpretability; vision-language models; sparse autoencoder; adaptation",
      "authors": "Hyesu Lim, Jinho Choi, Jaegul Choo, Steffen Schneider",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=imT03YXlG2",
      "openreview_forum_url": "https://openreview.net/forum?id=imT03YXlG2",
      "venue_id": "ICLR.cc/2025/Conference"
    },
    {
      "venue": "ICLR",
      "year": 2025,
      "title": "TabM：通过参数高效集成推进表格式深度学习",
      "title_en": "TabM: Advancing tabular deep learning with parameter-efficient ensembling",
      "abstract": "用于表格数据监督学习的深度学习架构从简单的多层感知器（MLP）到复杂的Transformer和检索增强方法。",
      "abstract_en": "Deep learning architectures for supervised learning on tabular data range from simple multilayer perceptrons (MLP) to sophisticated Transformers and retrieval-augmented methods.\nThis study highlights a major, yet so far overlooked opportunity for substantially improving tabular MLPs; namely, parameter-efficient ensembling -- a paradigm for imitating an ensemble of models with just one model.\nWe start by describing TabM -- a simple model based on MLP and BatchEnsemble (an existing technique), improved with our custom modifications.\nThen, we perform a large scale evaluation of tabular DL architectures on public benchmarks in terms of both task performance and efficiency, which renders the landscape of tabular DL in a new light.\nIn particular, we find that TabM outperforms prior tabular DL models, while the complexity of attention- and retrieval-based methods does not pay off.\nLastly, we conduct a detailed empirical analysis, that sheds some light on the high performance of TabM.\nFor example, we show that parameter-efficient ensembling is not an arbitrary trick, but rather a highly effective way to reduce overfitting and improve optimization dynamics of tabular MLPs.\nOverall, our work brings an impactful technique to tabular DL, analyses its behaviour, and advances the performance-efficiency tradeoff with TabM -- a simple and powerful baseline for researchers and practitioners.",
      "keywords": "tabular; tabular data; deep learning; architecture",
      "authors": "Yury Gorishniy, Akim Kotelnikov, Artem Babenko",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=Sd4wYYOhmY",
      "openreview_forum_url": "https://openreview.net/forum?id=Sd4wYYOhmY",
      "venue_id": "ICLR.cc/2025/Conference"
    },
    {
      "venue": "ICLR",
      "year": 2025,
      "title": "ToolGen：通过生成实现统一的工具检索和调用",
      "title_en": "ToolGen: Unified Tool Retrieval and Calling via Generation",
      "abstract": "随着大型语言模型（LLM）的进步，它们无法通过直接与外部工具交互来自主执行任务仍然是一个关键的限制。传统方法依赖于将工具描述作为上下文输入，这受到上下文长度的限制，需要单独的、通常效率低下的检索机制。我们引入了ToolGen，这是一种范式转变，通过将每个工具表示为唯一的令牌，将工具知识直接集成到LLM的参数中。这使得LLM能够生成工具调用和参数，作为其下一个令牌预测功能的一部分，将工具调用与语言生成无缝融合。我们的框架允许LLM访问和利用大量工具，而无需额外的检索步骤，从而显著提高了性能和可扩展性。使用47000多个工具的实验结果表明，ToolGen不仅在工具检索和自主任务完成方面取得了优异的结果，而且为能够适应不同领域工具的人工智能代理的新时代奠定了基础。通过将工具检索从根本上转变为生成过程，ToolGen为更通用、高效和自主的人工智能系统铺平了道路。ToolGen支持端到端的工具学习，并为与其他先进技术（如思维链和强化学习）的集成提供了机会，从而扩展了LLM的实践能力",
      "abstract_en": "As large language models (LLMs) advance, their inability to autonomously execute tasks by directly interacting with external tools remains a critical limitation. Traditional methods rely on inputting tool descriptions as context, which is constrained by context length and requires separate, often inefficient, retrieval mechanisms. We introduce ToolGen, a paradigm shift that integrates tool knowledge directly into the LLM’s parameters by representing each tool as a unique token. This enables the LLM to generate tool calls and arguments as part of its next token prediction capabilities, seamlessly blending tool invocation with language generation.  Our framework allows the LLM to access and utilize a vast amount of tools with no additional retrieval step, significantly enhancing both performance and scalability. Experimental results with over 47,000 tools show that ToolGen not only achieves superior results in both tool retrieval and autonomous task completion but also sets the stage for a new era of AI agents that can adapt to tools across diverse domains.  By fundamentally transforming tool retrieval into a generative process, ToolGen paves the way for more versatile, efficient, and autonomous AI systems. ToolGen enables end-to-end tool learning and opens opportunities for integration with other advanced techniques such as chain-of-thought and reinforcement learning, thereby expanding the practical capabilities of LLMs",
      "keywords": "Agent; Tool Learning; Virtual Token",
      "authors": "Renxi Wang, Xudong Han, Lei Ji, Shu Wang, Timothy Baldwin, Haonan Li",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=XLMAMmowdY",
      "openreview_forum_url": "https://openreview.net/forum?id=XLMAMmowdY",
      "venue_id": "ICLR.cc/2025/Conference"
    },
    {
      "venue": "ICLR",
      "year": 2025,
      "title": "视频动作差异",
      "title_en": "Video Action Differencing",
      "abstract": "两个人在执行同一动作时有何不同？在这项工作中，我们介绍了视频动作差异（VidDiff），这是一项识别同一动作视频之间细微差异的新任务，具有许多应用，如指导和技能学习。为了实现这项新任务的开发，我们首先创建了VidDiffBench，这是一个包含549个视频对的基准数据集，其中包含4469个细粒度动作差异的人工注释和2075个时间戳，指示这些差异发生的位置。我们的实验表明，VidDiffBench对GPT-4o和Qwen2 VL等最先进的大型多模态模型（LMM）构成了重大挑战。通过分析VidDiffBench上LMM的故障案例，我们强调了这项任务的两个关键挑战：在两个视频上定位相关子动作和细粒度帧比较。为了克服这些问题，我们提出了VidDiff方法，这是一种代理工作流，将任务分为三个阶段：动作差异建议、关键帧定位和帧差异，每个阶段都使用专门的基础模型。为了鼓励未来对这项新任务的研究，我们发布了基准测试和代码。",
      "abstract_en": "How do two individuals differ when performing the same action? In this work, we introduce Video Action Differencing (VidDiff), the novel task of identifying subtle differences between videos of the same action, which has numerous applications, such as coaching and skill learning. To enable development on this new task, we first create VidDiffBench, a benchmark dataset containing 549 video pairs, with human annotations of 4,469 fine-grained action differences and 2,075 timestamps indicating where these differences occur. Our experiments demonstrate that VidDiffBench poses a significant challenge for state-of-the-art large multimodal models (LMMs), such as GPT-4o and Qwen2-VL. By analyzing the failure cases of LMMs on VidDiffBench, we highlight two key challenges for this task: localizing relevant sub-actions over two videos and fine-grained frame comparison. To overcome these, we propose the VidDiff method, an agentic workflow that breaks the task into three stages: action difference proposal, keyframe localization, and frame differencing, each stage utilizing specialized foundation models. To encourage future research in this new task, we release the benchmark and code.",
      "keywords": "Video; Actions; Differencing; Zero-shot; benchmark; multimodal; lmm; llm",
      "authors": "James Burgess, Xiaohan Wang, Yuhui Zhang, Anita Rau, Alejandro Lozano, Lisa Dunlap, Trevor Darrell, Serena Yeung-Levy",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=3bcN6xlO6f",
      "openreview_forum_url": "https://openreview.net/forum?id=3bcN6xlO6f",
      "venue_id": "ICLR.cc/2025/Conference"
    },
    {
      "venue": "ICLR",
      "year": 2025,
      "title": "当GNN在ILP中满足对称性时：一种基于轨道的特征增强方法",
      "title_en": "When GNNs meet symmetry in ILPs: an orbit-based feature augmentation approach",
      "abstract": "整数线性规划（ILP）的一个共同特征是对称性，允许在不改变底层问题结构的情况下对变量进行置换。最近，GNN已经成为解决ILP的一种有前景的方法。 ",
      "abstract_en": "A common characteristic in integer linear programs (ILPs) is symmetry, allowing variables to be permuted without altering the underlying problem structure. Recently, GNNs have emerged as a promising approach for solving ILPs. \nHowever, a significant challenge arises when applying GNNs to ILPs with symmetry: classic GNN architectures struggle to differentiate between symmetric variables, which limits their predictive accuracy. In this work, we investigate the properties of permutation equivalence and invariance in GNNs, particularly in relation to the inherent symmetry of ILP formulations. We reveal that the interaction between these two factors contributes to the difficulty of distinguishing between symmetric variables.\nTo address this challenge, we explore the potential of feature augmentation and propose several guiding principles for constructing augmented features. Building on these principles, we develop an orbit-based augmentation scheme that first groups symmetric variables and then samples augmented features for each group from a discrete uniform distribution. Empirical results demonstrate that our proposed approach significantly enhances both training efficiency and predictive performance.",
      "keywords": "integer linear programming; symmetry; machine learning; graph neural networks",
      "authors": "Qian Chen, Lei Li, Qian Li, Jianghua Wu, Akang Wang, Ruoyu Sun, Xiaodong Luo, Tsung-Hui Chang, Qingjiang Shi",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=wVTJRnZ11Z",
      "openreview_forum_url": "https://openreview.net/forum?id=wVTJRnZ11Z",
      "venue_id": "ICLR.cc/2025/Conference"
    },
    {
      "venue": "NeurIPS",
      "year": 2025,
      "title": "CADGrasp：在杂乱场景中学习接触和碰撞感知的一般灵巧掌握",
      "title_en": "CADGrasp: Learning Contact and Collision Aware General Dexterous Grasping in Cluttered Scenes",
      "abstract": "由于灵巧手的高度自由度、遮挡以及由不同物体几何形状和复杂布局引起的潜在碰撞，在混乱的环境中灵巧抓握带来了巨大的挑战。为了应对这些挑战，我们提出了CADGrasp，这是一种使用单视点云输入进行一般灵巧抓取的两阶段算法。在第一阶段，我们预测一个场景解耦、接触和碰撞感知表示——稀疏IBS——作为优化目标。稀疏IBS紧凑地编码了灵巧手和场景之间的几何和接触关系，实现了稳定无碰撞的灵巧抓握姿势优化。为了增强这种高维表示的预测能力，我们引入了一种具有体素级条件引导和力闭合分数滤波的占用扩散模型。在第二阶段，我们开发了几种基于稀疏IBS的优化能量函数和排序策略，以生成高质量的灵巧抓握姿势。在模拟和现实环境中进行的大量实验验证了我们方法的有效性，证明了它在减轻碰撞的同时，在不同物体和复杂场景中保持高抓取成功率的能力。",
      "abstract_en": "Dexterous grasping in cluttered environments presents substantial challenges due to the high degrees of freedom of dexterous hands, occlusion, and potential collisions arising from diverse object geometries and complex layouts. To address these challenges, we propose CADGrasp, a two-stage algorithm for general dexterous grasping using single-view point cloud inputs. In the first stage, we predict a scene-decoupled, contact- and collision-aware representation—sparse IBS—as the optimization target. Sparse IBS compactly encodes the geometric and contact relationships between the dexterous hand and the scene, enabling stable and collision-free dexterous grasp pose optimization. To enhance the prediction of this high-dimensional representation, we introduce an occupancy-diffusion model with voxel-level conditional guidance and force closure score filtering. In the second stage, we develop several energy functions and ranking strategies for optimization based on sparse IBS to generate high-quality dexterous grasp poses. Extensive experiments in both simulated and real-world settings validate the effectiveness of our approach, demonstrating its capability to mitigate collisions while maintaining a high grasp success rate across diverse objects and complex scenes.",
      "keywords": "Dexterous Hand; General Grasping",
      "authors": "Jiyao Zhang, Zhiyuan Ma, Tianhao Wu, Zeyuan Chen, Hao Dong",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=CB8jwNE2vV",
      "openreview_forum_url": "https://openreview.net/forum?id=CB8jwNE2vV",
      "venue_id": "NeurIPS.cc/2025/Conference"
    },
    {
      "venue": "NeurIPS",
      "year": 2025,
      "title": "CVGL：因果学习与几何拓扑",
      "title_en": "CVGL: Causal Learning and Geometric Topology",
      "abstract": "交叉视图地理定位（CVGL）旨在通过将街道图像与相应的航空图像进行匹配来估计其地理位置。这对于复杂现实场景中的自主导航和地图绘制至关重要。然而，由于存在显著的观点差异和混杂因素的影响，这项任务仍然具有挑战性。为了解决这些问题，我们提出了因果学习和几何拓扑（CLGT）框架，该框架集成了两个关键组件：因果特征提取器（CFE），通过利用因果干预来鼓励模型专注于稳定的、与任务相关的语义，从而减轻混杂因素的影响；以及几何拓扑融合（GT Fusion）模块，该模块将鸟瞰图（BEV）道路拓扑注入街道特征，以缓解极端视角变化引起的交叉视图不一致。此外，我们引入了一个数据自适应池（DA池）模块来增强语义丰富区域的表示。对CVUSA、CVACT及其鲁棒性增强变体（CVUSA-C-ALL和CVACT-C-ALL）的广泛实验表明，CLGT实现了最先进的性能，特别是在具有挑战性的现实世界腐蚀下。",
      "abstract_en": "Cross-view geo-localization (CVGL) aims to estimate the geographic location of a street image by matching it with a corresponding aerial image. This is critical for autonomous navigation and mapping in complex real-world scenarios. However, the task remains challenging due to significant viewpoint differences and the influence of confounding factors. To tackle these issues, we propose the Causal Learning and Geometric Topology (CLGT) framework, which integrates two key components: a Causal Feature Extractor (CFE) that mitigates the influence of confounding factors by leveraging causal intervention to encourage the model to focus on stable, task-relevant semantics; and a Geometric Topology Fusion (GT Fusion) module that injects Bird’s Eye View (BEV) road topology into street features to alleviate cross-view inconsistencies caused by extreme perspective changes. Additionally, we introduce a Data-Adaptive Pooling (DA Pooling) module to enhance the representation of semantically rich regions. Extensive experiments on CVUSA, CVACT, and their robustness-enhanced variants (CVUSA-C-ALL and CVACT-C-ALL) demonstrate that CLGT achieves state-of-the-art performance, particularly under challenging real-world corruptions.",
      "keywords": "cross-view; casual learning; BEV",
      "authors": "Songsong Ouyang, Yingying Zhu",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=1CqEAuRzHc",
      "openreview_forum_url": "https://openreview.net/forum?id=1CqEAuRzHc",
      "venue_id": "NeurIPS.cc/2025/Conference"
    },
    {
      "venue": "NeurIPS",
      "year": 2025,
      "title": "用于高效超参数调整的成本敏感冻融贝叶斯优化",
      "title_en": "Cost-Sensitive Freeze-thaw Bayesian Optimization for Efficient Hyperparameter Tuning",
      "abstract": "本文研究了基于冻融贝叶斯优化（BO）的成本敏感超参数优化（HPO）问题。具体来说，我们假设一种情况，即当预期的性能改进在额外的计算成本方面不令人满意时，用户希望提前停止HPO过程。受此场景的启发，我们在冻融框架中引入了\\emph{utility}，这是一个描述成本和性能之间权衡的函数，可以根据用户的偏好数据进行估计。该效用函数与我们新颖的采集函数和停止标准相结合，使我们能够动态地继续训练我们期望在未来最大限度地提高效用的配置，并在最大效用附近自动停止HPO过程。此外，我们通过迁移学习提高了现有冻融方法的样本效率，为成本敏感的HPO问题开发了一个专门的替代模型。我们在已建立的多保真度HPO基准上验证了我们的算法，并表明它优于我们考虑的所有之前的冻融BO和转移BO基线，同时在成本和性能之间实现了更好的权衡。",
      "abstract_en": "In this paper, we address the problem of cost-sensitive hyperparameter optimization (HPO) built upon freeze-thaw Bayesian optimization (BO). Specifically, we assume a scenario where users want to early-stop the HPO process when the expected performance improvement is not satisfactory with respect to the additional computational cost. Motivated by this scenario, we introduce \\emph{utility} in the freeze-thaw framework, a function describing the trade-off between the cost and performance that can be estimated from the user's preference data. This utility function, combined with our novel acquisition function and stopping criterion, allows us to dynamically continue training the configuration that we expect to maximally improve the utility in the future, and also automatically stop the HPO process around the maximum utility. Further, we improve the sample efficiency of existing freeze-thaw methods with transfer learning to develop a specialized surrogate model for the cost-sensitive HPO problem. We validate our algorithm on established multi-fidelity HPO benchmarks and show that it outperforms all the previous freeze-thaw BO and transfer-BO baselines we consider, while achieving a significantly better trade-off between the cost and performance.",
      "keywords": "Cost-Sensitive; Bayesian Optimization; Multi-Fidelity HPO; PFNs; Transfer Learning",
      "authors": "Dong Bok Lee, Aoxuan Silvia Zhang, Byungjoo Kim, Junhyeon Park, Steven Adriaensen, Juho Lee, Sung Ju Hwang, Hae Beom Lee",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=ZUb4JpNoJe",
      "openreview_forum_url": "https://openreview.net/forum?id=ZUb4JpNoJe",
      "venue_id": "NeurIPS.cc/2025/Conference"
    },
    {
      "venue": "NeurIPS",
      "year": 2025,
      "title": "从欧拉到人工智能：统一数学常数公式",
      "title_en": "From Euler to AI: Unifying Formulas for Mathematical Constants",
      "abstract": "几个世纪以来，常数$\\large\\pi$一直吸引着学者，激发了无数计算公式，如无穷和和和连分式。尽管它们具有各自的重要性，但公式之间的许多潜在联系仍然未知，缺少可以揭示更深入理解的统一理论。缺乏统一的理论反映了数学和科学面临的更广泛的挑战：知识通常是通过孤立的发现积累的，而更深层次的联系往往是隐藏的。在这项工作中，我们提出了一个统一数学公式的自动化框架。我们的系统结合了用于系统公式收集的大型语言模型（LLM）、用于验证的LLM代码反馈循环以及用于聚类和最终统一的新型符号算法。我们在$\\large\\pi$的标志性案例上演示了这种方法，这是符号统一的理想测试场。将这种方法应用于455050篇arXiv论文，我们验证了$\\large\\pi$的385个不同公式，并证明了其中360个（94%）之间的关系，其中166个（43%）可以从一个数学对象中推导出来——欧拉、高斯、布隆克的正则公式，以及拉马努金机器算法发现的新公式。",
      "abstract_en": "The constant $\\large \\pi$ has fascinated scholars throughout the centuries, inspiring numerous formulas for its evaluation, such as infinite sums and continued fractions. Despite their individual significance, many of the underlying connections among formulas remain unknown, missing unifying theories that could unveil deeper understanding. The absence of a unifying theory reflects a broader challenge across math and science: knowledge is typically accumulated through isolated discoveries, while deeper connections often remain hidden. In this work, we present an automated framework for the unification of mathematical formulas. Our system combines large language models (LLMs) for systematic formula harvesting, an LLM-code feedback loop for validation, and a novel symbolic algorithm for clustering and eventual unification. We demonstrate this methodology on the hallmark case of $\\large \\pi$, an ideal testing ground for symbolic unification. Applying this approach to 455,050 arXiv papers, we validate 385 distinct formulas for $\\large \\pi$ and prove relations between 360 (94\\%) of them, of which 166 (43\\%) can be derived from a single mathematical object—linking canonical formulas by Euler, Gauss, Brouncker, and newer ones from algorithmic discoveries by the Ramanujan Machine.\n  Our method generalizes to other constants, including $e$, $\\zeta(3)$, and Catalan’s constant, demonstrating the potential of AI-assisted mathematics to uncover hidden structures and unify knowledge across domains.",
      "keywords": "AI for Science; AI for Math; LLM-Tool Integration; Mathematical Constants; Continued Fractions; Recurrences; Number Theory; Pi",
      "authors": "Tomer Raz, Michael Shalyt, Elyasheev Leibtag, Rotem Kalisch, Shachar Weinbaum, Yaron Hadad, Ido Kaminer",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=cNqMAmpZh4",
      "openreview_forum_url": "https://openreview.net/forum?id=cNqMAmpZh4",
      "venue_id": "NeurIPS.cc/2025/Conference"
    },
    {
      "venue": "NeurIPS",
      "year": 2025,
      "title": "目标阶梯：使用视觉语言模型进行增量目标发现",
      "title_en": "GoalLadder: Incremental Goal Discovery with Vision-Language Models",
      "abstract": "自然语言可以提供一种简洁且人类可解释的方法来指定强化学习（RL）任务。从语言指令中提取奖励的能力可以开发出可以从人类指导中学习的机器人系统；然而，这仍然是一个具有挑战性的问题，尤其是在视觉环境中。采用大型预训练语言模型的现有方法要么依赖于非视觉环境表示，需要大量的反馈，要么生成嘈杂、形状不好的奖励函数。在这篇论文中，我们提出了一种新的方法GoalLadder，该方法利用视觉语言模型（VLM）在视觉环境中从单一语言指令中训练RL代理。GoalLadder的工作原理是逐步发现状态，使代理更接近完成自然语言中指定的任务。为此，它查询VLM以识别代表代理任务进度改进的状态，并使用成对比较对其进行排名。与之前的工作不同，GoalLadder并不完全信任VLM的反馈；相反，它使用它来使用基于ELO的评级系统对潜在的目标状态进行排名，从而减少噪声VLM反馈的不利影响。在训练过程中，代理的任务是在学习的嵌入空间中最小化与排名靠前的目标的距离，该嵌入空间是在未标记的视觉数据上训练的。这一关键特性使我们能够绕过训练形状良好的奖励函数通常所需的大量准确反馈的需求。我们证明，GoalLadder在经典控制和机器人操纵环境中的表现优于现有的相关方法，平均最终成功率为95%，而最佳竞争对手的成功率仅为45%。",
      "abstract_en": "Natural language can offer a concise and human-interpretable means of specifying reinforcement learning (RL) tasks. The ability to extract rewards from a language instruction can enable the development of robotic systems that can learn from human guidance; however, it remains a challenging problem, especially in visual environments. Existing approaches that employ large, pretrained language models either rely on non‐visual environment representations, require prohibitively large amounts of feedback, or generate noisy, ill‐shaped reward functions. In this paper, we propose a novel method, GoalLadder, that leverages vision-language models (VLMs) to train RL agents from a single language instruction in visual environments. GoalLadder works by incrementally discovering states that bring the agent closer to completing a task specified in natural language. To do so, it queries a VLM to identify states that represent an improvement in agent's task progress and to rank them using pairwise comparisons. Unlike prior work, GoalLadder does not trust VLM's feedback completely; instead, it uses it to rank potential goal states using an ELO-based rating system, thus reducing the detrimental effects of noisy VLM feedback. Over the course of training, the agent is tasked with minimising the distance to the top-ranked goal in a learned embedding space, which is trained on unlabelled visual data. This key feature allows us to bypass the need for abundant and accurate feedback typically required to train a well-shaped reward function. We demonstrate that GoalLadder outperforms existing related methods on classic control and robotic manipulation environments with the average final success rate of $\\sim$95\\% compared to only $\\sim$45\\% of the best competitor.",
      "keywords": "reinforcement learning; vision-language models",
      "authors": "Alexey Zakharov, Shimon Whiteson",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=BiowiwzQaO",
      "openreview_forum_url": "https://openreview.net/forum?id=BiowiwzQaO",
      "venue_id": "NeurIPS.cc/2025/Conference"
    },
    {
      "venue": "NeurIPS",
      "year": 2025,
      "title": "HM3：预训练模型的分层多目标模型合并",
      "title_en": "HM3: Hierarchical Multi-Objective Model Merging for Pretrained Models",
      "abstract": "模型合并是一种技术，它将多个大型预训练模型组合成一个模型，在没有原始数据或额外训练的情况下提高性能并扩大任务适应性。然而，大多数现有的模型合并方法主要侧重于探索参数空间，合并具有相同架构的模型。尽管有潜力，但由于巨大的搜索空间和与层兼容性相关的挑战，架构领域的合并仍处于早期阶段。本文设计了一个名为HM3的分层模型合并框架，提出了一个跨参数和架构空间的双层多目标模型合并问题。在参数级别，HM3集成了现有的合并方法，以快速识别最佳参数。在此基础上，在架构级别采用具有高效策略离散化的行动者批评策略，在层粒度搜索空间中探索具有马尔可夫特性的推理路径，以重建这些最优模型。通过训练可重用的策略和价值网络，HM3学习帕累托最优模型，为各种任务提供定制的解决方案。语言和视觉任务的实验结果表明，HM3优于仅关注参数或架构空间的方法。",
      "abstract_en": "Model merging is a technique that combines multiple large pretrained models into a single model, enhancing performance and broadening task adaptability without original data or additional training. However, most existing model merging methods focus primarily on exploring the parameter space, merging models with identical architectures. Despite its potential, merging in the architecture space remains in its early stages due to the vast search space and challenges related to layer compatibility. This paper designs a hierarchical model merging framework named HM3, formulating a bilevel multi-objective model merging problem across both parameter and architecture spaces. At the parameter level, HM3 integrates existing merging methods to quickly identify optimal parameters. Based on these, an actor-critic strategy with efficient policy discretization is employed at the architecture level to explore inference paths with Markov property in the layer-granularity search space for reconstructing these optimal models. By training reusable policy and value networks, HM3 learns Pareto optimal models to provide customized solutions for various tasks. Experimental results on language and vision tasks demonstrate that HM3 outperforms methods focusing solely on the parameter or architecture space.",
      "keywords": "Large language model; model merging; multi-objective optimization; architecture-level merging",
      "authors": "Yu Zhou, Xingyu Wu, Jibin Wu, Liang Feng, KC Tan",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=JeP0lpusYw",
      "openreview_forum_url": "https://openreview.net/forum?id=JeP0lpusYw",
      "venue_id": "NeurIPS.cc/2025/Conference"
    },
    {
      "venue": "NeurIPS",
      "year": 2025,
      "title": "边缘有色超图重叠和鲁棒聚类的改进算法：基于LP的组合方法",
      "title_en": "Improved Algorithms for Overlapping and Robust Clustering of Edge-Colored Hypergraphs: An LP-Based Combinatorial Approach",
      "abstract": "聚类是机器学习和数据挖掘中的一项基本任务。在各种方法中，边缘着色聚类（ECC）已成为处理分类数据的一种有用方法。给定一个具有由颜色标记的（超）边的超图，ECC旨在分配顶点颜色，以最小化顶点颜色与边颜色不同的边的数量。然而，传统ECC具有固有的局限性，因为它强制执行非重叠和详尽的聚类。为了解决这些局限性，研究了三种版本的ECC：允许重叠聚类的局部ECC和全局ECC，以及考虑顶点异常值的鲁棒ECC。针对这些问题，已经提出了线性规划（LP）舍入算法和贪婪组合算法。虽然这些LP舍入算法提供了高质量的解决方案，但它们需要大量的计算时间；另一方面，贪婪算法运行速度很快，但往往会影响解决方案的质量。在本文中，我们提出了一个算法家族，该家族将LP的优势与组合算法的计算效率相结合。实验和理论分析都表明，我们的算法有效地为所有三个问题（局部、全局和鲁棒ECC）提供了高质量的解决方案。我们用复杂性理论的不可近似性结果和完整性缺口界限来补充我们的算法贡献，这表明不太可能进行重大的理论改进。我们的研究结果还回答了文献中提出的两个悬而未决的问题。",
      "abstract_en": "Clustering is a fundamental task in both machine learning and data mining. Among various methods, edge-colored clustering (ECC) has emerged as a useful approach for handling categorical data. Given a hypergraph with (hyper)edges labeled by colors, ECC aims to assign vertex colors to minimize the number of edges where the vertex color differs from the edge's color. However, traditional ECC has inherent limitations, as it enforces a nonoverlapping and exhaustive clustering. To tackle these limitations, three versions of ECC have been studied: Local ECC and Global ECC, which allow overlapping clusters, and Robust ECC, which accounts for vertex outliers. For these problems, both linear programming (LP) rounding algorithms and greedy combinatorial algorithms have been proposed. While these LP-rounding algorithms provide high-quality solutions, they demand substantial computation time; the greedy algorithms, on the other hand, run very fast but often compromise solution quality. In this paper, we present a family of algorithms that combines the strengths of LP with the computational efficiency of combinatorial algorithms. Both experimental and theoretical analyses show that our algorithms efficiently produce high-quality solutions for all three problems: Local, Global, and Robust ECC. We complement our algorithmic contributions with complexity-theoretic inapproximability results and integrality gap bounds, which suggest that significant theoretical improvements are unlikely. Our results also answer two open questions previously raised in the literature.",
      "keywords": "overlapping edge-colored clustering; robust edge-colored clustering; edge-colored clustering; hypergraph clustering; primal-dual methods; approximation algorithms",
      "authors": "Changyeol Lee, Yongho Shin, Hyung-Chan An",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=F3DrgOZYc6",
      "openreview_forum_url": "https://openreview.net/forum?id=F3DrgOZYc6",
      "venue_id": "NeurIPS.cc/2025/Conference"
    },
    {
      "venue": "NeurIPS",
      "year": 2025,
      "title": "信息论离散扩散",
      "title_en": "Information-Theoretic Discrete Diffusion",
      "abstract": "我们提出了离散扩散模型的信息论框架",
      "abstract_en": "We present an information-theoretic framework for discrete diffusion models \nthat yields principled estimators of log-likelihood using score-matching losses.\nInspired by the I-MMSE identity for the Gaussian setup, we derive analogous results for the discrete setting.\nSpecifically, we introduce the Information–Minimum Denoising Score Entropy (I-MDSE) relation,\nwhich links mutual information between data and its diffused version to the minimum denoising score entropy (DSE) loss.\nWe extend this theory to masked diffusion and establish the Information–Minimum Denoising Cross-Entropy (I-MDCE) relation,\nconnecting cross-entropy losses to mutual information in discrete masked processes.\nThese results provide a time-integral decomposition of the log-likelihood of the data in terms of optimal score-based losses,\nshowing that commonly used losses such as DSE and DCE are not merely variational bounds \nbut tight and principled estimators of log-likelihood.\nThe I-MDCE decomposition further enables practical extensions, including time-free formula,\nconditional likelihood estimation in prompt–response tasks, and coupled Monte Carlo estimation of likelihood ratios.\nExperiments on synthetic and real-world data confirm the accuracy, variance stability, and utility of our estimators.\nThe code is publicly available at https://github.com/Dongjae0324/infodis.",
      "keywords": "Discrete Diffusion Models; Information Theory; Score Matching; Denoising Score Entropy (DSE); Denoising Cross-Entropy (DCE)",
      "authors": "Moongyu Jeon, Sangwoo Shin, Dongjae Jeon, Albert No",
      "github_links": "https://github.com/Dongjae0324/infodis",
      "pdf_download_url": "https://openreview.net/pdf?id=B2iPEX5A9c",
      "openreview_forum_url": "https://openreview.net/forum?id=B2iPEX5A9c",
      "venue_id": "NeurIPS.cc/2025/Conference"
    },
    {
      "venue": "NeurIPS",
      "year": 2025,
      "title": "KLASS：掩蔽扩散模型中KL引导的快速推理",
      "title_en": "KLASS: KL-Guided Fast Inference in Masked Diffusion Models",
      "abstract": "掩蔽扩散模型在包括语言生成在内的各种任务上都显示出了具有竞争力的结果。然而，由于其迭代细化过程，推理往往受到缓慢和静态采样速度的阻碍。为了克服这个问题，我们引入了“KL自适应稳定性采样”（KLASS），这是一种快速而有效的采样方法，利用令牌级KL散度来识别稳定、高置信度的预测。通过在每次迭代中揭开多个令牌，而无需任何额外的模型训练，我们的方法在保持样本质量的同时显著加快了生成速度。在推理基准测试中，KLASS实现了高达2.78美元的时钟加速，同时提高了标准贪婪解码的性能，在基于扩散的采样器中获得了最先进的结果。我们进一步验证了KLASS在不同领域的有效性，包括文本、图像和分子生成，表明它是一种适用于不同模型的广泛采样器。",
      "abstract_en": "Masked diffusion models have demonstrated competitive results on various tasks including language generation. However, due to its iterative refinement process, the inference is often bottlenecked by slow and static sampling speed. To overcome this problem, we introduce `KL-Adaptive Stability Sampling' (KLASS), a fast yet effective sampling method that exploits token-level KL divergence to identify stable, high-confidence predictions. By unmasking multiple tokens in each iteration without any additional model training, our approach speeds up generation significantly while maintaining sample quality. On reasoning benchmarks, KLASS achieves up to $2.78\\times$ wall-clock speedups while improving performance over standard greedy decoding, attaining state-of-the-art results among diffusion-based samplers. We further validate KLASS across diverse domains, including text, image, and molecular generation, showing its effectiveness as a broadly applicable sampler across different models.",
      "keywords": "Generative Models; Efficient Inference Methods",
      "authors": "Seo Hyun Kim, Sunwoo Hong, Hojung Jung, Youngrok Park, Se-Young Yun",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=gOG9Zoyn4R",
      "openreview_forum_url": "https://openreview.net/forum?id=gOG9Zoyn4R",
      "venue_id": "NeurIPS.cc/2025/Conference"
    },
    {
      "venue": "NeurIPS",
      "year": 2025,
      "title": "ModHiFi：识别模型修改的高保真预测组件",
      "title_en": "ModHiFi: Identifying High Fidelity predictive components for Model Modification",
      "abstract": "在不访问训练数据或原始损失函数的情况下，出于修剪或遗忘等目的修改训练有素的模型是一个具有挑战性的问题。虽然存在这种修改的技术，但它们通常需要训练数据，计算成本很高，或者是特定于架构的。为了解决这个问题，我们研究了识别对模型预测性能至关重要的组件的基本问题，而无需访问梯度或损失函数，只需使用合成数据等分布式访问。 ",
      "abstract_en": "Modifying well-trained models for purposes such as pruning or unlearning, without access to training data or the original loss function, is a challenging problem. While techniques exist for such modification, they often require training data, are computationally expensive, or are architecture-specific. To address this, we investigate the fundamental question of identifying components that are critical to the model’s predictive performance, without access to either gradients or the loss function, and with only distributional access such as synthetic data. \n    We theoretically demonstrate that the global reconstruction error is linearly bounded by local reconstruction errors for Lipschitz-continuous networks such as CNNs and well-trained Transformers (which, contrary to existing literature, we find exhibit Lipschitz continuity). This motivates using the locally reconstructive behavior of component subsets to quantify their global importance, via a metric that we term *Subset Fidelity*. In the uncorrelated features setting, selecting individual components via their Subset Fidelity scores is optimal, which we use to propose **ModHiFi**, an algorithm for model modification that requires no training data or loss function access. **ModHiFi-P**, for structured pruning, achieves an 11% speedup over the current state of the art on ImageNet models and competitive performance on language models. **ModHiFi-U**, for classwise unlearning, achieves complete unlearning on CIFAR-10 without fine-tuning and demonstrates competitive performance on Swin Transformers.",
      "keywords": "Pruning; Machine Unlearning",
      "authors": "Dhruva Kashyap, Chaitanya Murti, Pranav K Nayak, Tanay Narshana, Chiranjib Bhattacharyya",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=lClK4uBxSG",
      "openreview_forum_url": "https://openreview.net/forum?id=lClK4uBxSG",
      "venue_id": "NeurIPS.cc/2025/Conference"
    },
    {
      "venue": "NeurIPS",
      "year": 2025,
      "title": "通过正交性进行新的探索",
      "title_en": "Novel Exploration via Orthogonality",
      "abstract": "有效的勘探仍然是加固中的关键问题之一",
      "abstract_en": "Efficient exploration remains one of the key open problems in reinforcement\nlearning. Discovering novel states or transitions efficiently requires policies that\neffectively direct the agent away from regions of the state space that are already\nwell explored. We introduce Novel Exploration via Orthogonality (NEO), an\napproach that automatically uncovers not only which regions of the environment\nare novel but also how to reach them by leveraging Laplacian representations. NEO\nuses the eigenvectors of a modified graph Laplacian to induce gradient flows from\nstates that are frequently visited (less novel) to states that are seldom visited (more\nnovel). We show that NEO’s modified Laplacian yields eigenvectors whose extreme\nvalues align with the most novel regions of the state space. We provide bounds\nfor the eigenvalues of the modified Laplacian; and we show that the smoothest\neigenvectors with real eigenvalues below certain thresholds provide guaranteed\ngradients to novel states for both undirected and directed graphs. In an empirical\nevaluation in online, incremental settings, NEO outperformed related state-of-the-\nart approaches, including eigen-options and cover options, in a large collection of\nundirected and directed domains with varying structures.",
      "keywords": "Laplacian; Novelty; Reinforcement Learning; Exploration; Eigenvectors; Spectral Methods",
      "authors": "Andreas Theophilou, Özgür Şimşek",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=yJS1eZSNUv",
      "openreview_forum_url": "https://openreview.net/forum?id=yJS1eZSNUv",
      "venue_id": "NeurIPS.cc/2025/Conference"
    },
    {
      "venue": "NeurIPS",
      "year": 2025,
      "title": "Q-Palette：分数比特量化器实现高效LLM部署的最佳比特分配",
      "title_en": "Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for Efficient LLM Deployment",
      "abstract": "我们研究了仅使用权重的训练后量化（PTQ），它使用很少或没有校准数据，在不进行再训练的情况下量化大型语言模型（LLM）的权重。仅权重PTQ对于减少LLM推理的内存占用和延迟至关重要，特别是在内存受限的小批量推理场景中，例如边缘设备上的个性化推理。尽管它很重要，但LLM中具有重尾异常值的不规则权重分布使量化复杂化，最近提出了基于旋转的方法，将权重转换为近高斯分布，这种分布更规则，异常值更少，从而减少了量化误差。在这项工作中，我们首先推导了给定比特预算下高斯化权重的理论最优比特分配信息，揭示了接近高斯失真率界限的细粒度分数比特量化器对于实现接近最优的量化性能至关重要。为了将这一理论见解与实际实现联系起来，我们引入了Q-Palette，这是一个多功能的分数比特量化器集合，从提供接近最佳失真的网格编码量化器到为更快推理而优化的更简单的向量和标量量化器，所有这些都通过跨各种位宽的优化CUDA内核高效实现。此外，利用Q-Palette作为基础组件，我们提出了一种新的混合方案量化框架，在资源约束的情况下联合优化量化器选择和层融合决策。该代码可在以下网址获得https://github.com/snu-mllab/Q-Palette.",
      "abstract_en": "We study weight-only post-training quantization (PTQ), which quantizes the weights of a large language model (LLM) without retraining, using little or no calibration data. Weight-only PTQ is crucial for reducing the memory footprint and latency of LLM inference, especially in memory-bound, small-batch inference scenarios, such as personalized inference on edge devices. Despite its importance, irregular weight distributions with heavy-tailed outliers in LLMs complicate quantization, recently motivating rotation-based methods that transform weights into near-Gaussian distributions, which are more regular with fewer outliers, thereby reducing quantization error. In this work, we first derive the information-theoretically optimal bit allocation for Gaussianized weights under given bit budgets, revealing that fine-grained fractional-bit quantizers approaching the Gaussian distortion-rate bound are essential to achieve near-optimal quantization performance. To bridge this theoretical insight and practical implementation, we introduce Q-Palette, a versatile collection of fractional-bit quantizers that range from trellis-coded quantizers offering near-optimal distortion to simpler vector and scalar quantizers optimized for faster inference, all efficiently implemented with optimized CUDA kernels across various bitwidths. Furthermore, leveraging Q-Palette as a foundational component, we propose a novel mixed-scheme quantization framework, jointly optimizing quantizer choices and layer fusion decisions given resource constraints. The code is available at https://github.com/snu-mllab/Q-Palette.",
      "keywords": "LLM quantization; Post-training quantization; Mixed scheme quantization; Data-free quantization",
      "authors": "Deokjae Lee, Hyun Oh Song",
      "github_links": "https://github.com/snu-mllab/Q-Palette",
      "pdf_download_url": "https://openreview.net/pdf?id=l4F50jpiVH",
      "openreview_forum_url": "https://openreview.net/forum?id=l4F50jpiVH",
      "venue_id": "NeurIPS.cc/2025/Conference"
    },
    {
      "venue": "NeurIPS",
      "year": 2025,
      "title": "REVE:25000名受试者大规模预训练的EEG基础模型",
      "title_en": "REVE: A Foundation Model for EEG - Adapting to Any Setup with Large-Scale Pretraining on 25,000 Subjects",
      "abstract": "基础模型通过大规模预训练减少了对特定任务数据的依赖，从而改变了人工智能。虽然在语言和视觉方面取得了成功，但由于公共数据集的异质性，它们在EEG中的采用却滞后了，这些数据集是在不同的协议、设备和电极配置下收集的。现有的EEG基础模型难以在这些变化中推广，通常将预训练限制在单个设置中，导致性能不佳，特别是在线性探测下。",
      "abstract_en": "Foundation models have transformed AI by reducing reliance on task-specific data through large-scale pretraining. While successful in language and vision, their adoption in EEG has lagged due to the heterogeneity of public datasets, which are collected under varying protocols, devices, and electrode configurations. Existing EEG foundation models struggle to generalize across these variations, often restricting pretraining to a single setup, resulting in suboptimal performance, in particular under linear probing.\nWe present REVE (Representation for EEG with Versatile Embeddings), a pretrained model explicitly designed to generalize across diverse EEG signals. REVE introduces a novel 4D positional encoding scheme that enables it to process signals of arbitrary length and electrode arrangement. Using a masked autoencoding objective, we pretrain REVE on over 60,000 hours of EEG data from 92 datasets spanning 25,000 subjects, representing the largest EEG pretraining effort to date.\nREVE achieves state-of-the-art results on 10 downstream EEG tasks, including motor imagery classification, seizure detection, sleep staging, cognitive load estimation, and emotion recognition. With little to no fine-tuning, it demonstrates strong generalization, and nuanced spatio-temporal modeling. We release code, pretrained weights, and tutorials to support standardized EEG research and accelerate progress in clinical neuroscience.",
      "keywords": "Foundation Model; EEG; SSL; BCI",
      "authors": "Yassine El Ouahidi, Jonathan Lys, Philipp Thölke, Nicolas Farrugia, Bastien Pasdeloup, Vincent Gripon, Karim Jerbi, Giulia Lioi",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=ZeFMtRBy4Z",
      "openreview_forum_url": "https://openreview.net/forum?id=ZeFMtRBy4Z",
      "venue_id": "NeurIPS.cc/2025/Conference"
    },
    {
      "venue": "NeurIPS",
      "year": 2025,
      "title": "Router-R1：通过强化学习教授LLM多轮路由和聚合",
      "title_en": "Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning",
      "abstract": "各种大型语言模型（LLM）的快速出现刺激了LLM路由器的发展，这些路由器将用户查询分配给最合适的模型。然而，现有的LLM路由器通常执行单轮一对一映射（\\textit{即}，将每个查询单独分配给一个模型），这限制了它们处理需要多个LLM互补优势的复杂任务的能力。在本文中，我们提出了\\textbf{Router-R1}，这是一个基于强化学习（RL）的框架，它将多LLM路由和聚合表示为一个顺序决策过程。Router-R1将路由器本身实例化为一个有能力的LLM，利用其推理能力将“思考”动作（内部审议）与“路由”动作（动态模型调用）交织在一起，并将每个响应集成到其不断发展的上下文中。为了促进学习，我们采用了一种轻量级的基于规则的奖励，包括格式奖励、最终结果奖励和一种新的成本奖励，以优化性能和成本之间的平衡，为通过RL增强性能成本权衡开辟了一条道路。Router-R1也只对定价、延迟和示例性能等简单的模型描述符进行条件限制，从而能够对看不见的模型选择进行强有力的泛化。在七个通用和多跳QA基准上的实验表明，Router-R1优于几个强基线，在保持鲁棒泛化和成本管理的同时实现了卓越的性能。",
      "abstract_en": "The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\\textit{i.e.}, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \\textbf{Router-R1}, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave \"think\" actions (internal deliberation) with \"route\" actions (dynamic model invocation), and integrates each response into its evolving context. To facilitate learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for optimizing the balance between performance and cost, opening a pathway toward enhancing performance-cost trade-offs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms several strong baselines, achieving superior performance while maintaining robust generalization and cost management.",
      "keywords": "Large Language Models; LLM Routers; LLM Selection; Reinforcement Learning",
      "authors": "Haozhen Zhang, Tao Feng, Jiaxuan You",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=DWf4vroKWJ",
      "openreview_forum_url": "https://openreview.net/forum?id=DWf4vroKWJ",
      "venue_id": "NeurIPS.cc/2025/Conference"
    },
    {
      "venue": "NeurIPS",
      "year": 2025,
      "title": "通过均匀边缘采样实现结构感知的光谱稀疏化",
      "title_en": "Structure-Aware Spectral Sparsification via Uniform Edge Sampling",
      "abstract": "谱聚类是图划分的一种基本方法，但它对特征向量计算的依赖限制了其对海量图的可扩展性。经典的稀疏化方法通过按有效电阻成比例采样边缘来保持光谱特性，但需要昂贵的预处理来估计这些电阻。我们研究了均匀边缘采样（一种简单的、与结构无关的策略）是否足以用于谱聚类。我们的主要结果表明，对于允许良好分离的$k$-聚类的图，其特征是大的结构比$\\Upsilon（k）=\\lambda_{k+1}/\\rho_G（k）$，均匀采样保留了用于聚类的谱子空间。具体来说，我们证明了均匀采样$O（\\gamma^2 n\\log n/\\varepsilon^2）$边缘，其中$\\gamma$是拉普拉斯条件数，会产生一个稀疏化器，其顶部$（n-k）$维特征空间与聚类指标近似正交。这确保了光谱嵌入保持忠实，并保留了聚类质量。我们的分析引入了簇内边的新阻力界限、秩-（n-k）-有效阻力公式和适用于主导特征空间的矩阵Chernoff界限。这些工具使我们能够完全绕过重要性抽样。从概念上讲，我们的结果将最近的基于核集的聚类理论与光谱稀疏化联系起来，表明在强聚类能力下，即使是均匀采样也是结构感知的。这提供了第一个可证明的保证，即均匀边缘采样足以用于结构保持的谱聚类。",
      "abstract_en": "Spectral clustering is a fundamental method for graph partitioning, but its reliance on eigenvector computation limits scalability to massive graphs. Classical sparsification methods preserve spectral properties by sampling edges proportionally to their effective resistances, but require expensive preprocessing to estimate these resistances. We study whether uniform edge sampling—a simple, structure-agnostic strategy—can suffice for spectral clustering. Our main result shows that for graphs admitting a well-separated $k$-clustering, characterized by a large structure ratio $\\Upsilon(k) = \\lambda_{k+1} / \\rho_G(k)$, uniform sampling preserves the spectral subspace used for clustering. Specifically, we prove that uniformly sampling $O(\\gamma^2 n \\log n / \\varepsilon^2)$ edges, where $\\gamma$ is the Laplacian condition number, yields a sparsifier whose top $(n-k)$-dimensional eigenspace is approximately orthogonal to the cluster indicators. This ensures that the spectral embedding remains faithful, and clustering quality is preserved. Our analysis introduces new resistance bounds for intra-cluster edges, a rank-$(n-k)$ effective resistance formulation, and a matrix Chernoff bound adapted to the dominant eigenspace. These tools allow us to bypass importance sampling entirely. Conceptually, our result connects recent coreset-based clustering theory to spectral sparsification, showing that under strong clusterability, even uniform sampling is structure-aware. This provides the first provable guarantee that uniform edge sampling suffices for structure-preserving spectral clustering.",
      "keywords": "Spectral Clustering; Graph Sparsification",
      "authors": "Kaiwen He, Petros Drineas, Rajiv Khanna",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=Z4eFqgYbha",
      "openreview_forum_url": "https://openreview.net/forum?id=Z4eFqgYbha",
      "venue_id": "NeurIPS.cc/2025/Conference"
    },
    {
      "venue": "NeurIPS",
      "year": 2025,
      "title": "好、坏、丑：水印、可转移攻击和对抗防御的元分析",
      "title_en": "The Good, the Bad and the Ugly: Meta-Analysis of Watermarks, Transferable Attacks and Adversarial Defenses",
      "abstract": "我们形式化并分析了基于后门的水印和对抗性防御之间的权衡，将其构建为验证者和证明者之间的交互协议。虽然之前的工作主要集中在这种权衡上，但我们的分析通过将可转移的攻击识别为第三种违反直觉但必要的选择来扩展它。我们的主要结果表明，对于所有学习任务，至少存在以下三种中的一种：水印、敌意防御或可转移攻击。通过可转移攻击，我们指的是一种高效的算法，该算法生成的查询与数据分布无法区分，并且能够欺骗所有高效的防御者。使用密码技术，特别是完全同态加密，我们构建了一个可转移的攻击，并证明了它在这种权衡中的必要性。最后，我们证明了有界VC维度的任务允许对所有攻击者进行对抗性防御，而子类则允许水印对快速对手进行安全防护。",
      "abstract_en": "We formalize and analyze the trade-off between backdoor-based watermarks and adversarial defenses, framing it as an interactive protocol between a verifier and a prover. While previous works have primarily focused on this trade-off, our analysis extends it by identifying transferable attacks as a third, counterintuitive but necessary option. Our main result shows that for all learning tasks, at least one of the three exists: a _watermark_, an _adversarial defense_, or a _transferable attack_. By transferable attack, we refer to an efficient algorithm that generates queries indistinguishable from the data distribution and capable of fooling _all_ efficient defenders. Using cryptographic techniques, specifically fully homomorphic encryption, we construct a transferable attack and prove its necessity in this trade-off. Finally, we show that tasks of bounded VC-dimension allow adversarial defenses against all attackers, while a subclass allows watermarks secure against fast adversaries.",
      "keywords": "Interactive Proof Systems; Cryptography; Backdoors; Game Theory; Learning Theory; Transferable Attacks; Adversarial Robustness",
      "authors": "Grzegorz Gluch, Berkant Turan, Sai Ganesh Nagarajan, Sebastian Pokutta",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=NVDrWBwJTV",
      "openreview_forum_url": "https://openreview.net/forum?id=NVDrWBwJTV",
      "venue_id": "NeurIPS.cc/2025/Conference"
    },
    {
      "venue": "NeurIPS",
      "year": 2025,
      "title": "大型语言模型中关系解码线性算子的结构",
      "title_en": "The Structure of Relation Decoding Linear Operators in Large Language Models",
      "abstract": "本文研究了Hernandez等人[2023]中引入的线性算子的结构，该算子用于解码变压器语言模型中的特定关系事实。我们将他们的单一关系发现扩展到一系列关系，并系统地绘制他们的组织结构图。我们证明，这种关系解码器的集合可以通过简单的3阶张量网络进行高度压缩，而不会显著降低解码精度。为了解释这种令人惊讶的冗余，我们开发了一种交叉评估协议，在该协议中，我们将每个线性解码器运算符应用于其他关系的主题。我们的结果表明，这些线性映射不编码不同的关系，而是提取重复的粗粒度语义属性（例如，首都国家和食品国家都属于X国属性）。这种以属性为中心的结构阐明了运算符的可压缩性，并强调了为什么它们只泛化到语义上接近的新关系。因此，我们的研究结果将转换器语言模型中的线性关系解码解释为主要基于属性，而不是特定于关系。",
      "abstract_en": "This paper investigates the structure of linear operators introduced in Hernandez et al. [2023] that decode specific relational facts in transformer language models. We extend their single-relation findings to a collection of relations and systematically chart their organization. We show that such collections of relation decoders can be highly compressed by simple order-3 tensor networks without significant loss in decoding accuracy. To explain this surprising redundancy, we develop a cross-evaluation protocol, in which we apply each linear decoder operator to the subjects of every other relation. Our results reveal that these linear maps do not encode distinct relations, but extract recurring, coarse-grained semantic properties (e.g., country of capital city and country of food are both in the country-of-X property). This property-centric structure clarifies both the operators' compressibility and highlights why they generalize only to new relations that are semantically close. Our findings thus interpret linear relational decoding in transformer language models as primarily property-based, rather than relation-specific.",
      "keywords": "large language models; relations; tensor networks; interpretability",
      "authors": "Miranda Anna Christ, Adrián Csiszárik, Gergely Becsó, Dániel Varga",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=XsBzmJzJ2l",
      "openreview_forum_url": "https://openreview.net/forum?id=XsBzmJzJ2l",
      "venue_id": "NeurIPS.cc/2025/Conference"
    },
    {
      "venue": "NeurIPS",
      "year": 2025,
      "title": "误差反馈一阶方法的严密分析",
      "title_en": "Tight analyses of first-order methods with error feedback",
      "abstract": "代理之间的通信通常构成分布式学习中的主要计算瓶颈。最常见的缓解策略之一是压缩交换的信息，从而减少通信开销。为了抵消与压缩通信相关的收敛性下降，引入了错误反馈方案，最明显的是EF和EF21。在这项工作中，我们对这两种方法进行了深入分析。具体来说，我们找到了李雅普诺夫函数，该函数为每种方法提供了最佳的收敛速度，并具有匹配的下限。这种原则性的方法提供了清晰的性能保证，并能够对EF、EF21和压缩梯度下降进行严格的比较。我们的分析是在简化的单代理设置中进行的，这允许对潜在机制进行清晰的理论见解和公平的比较。",
      "abstract_en": "Communication between agents often constitutes a major computational bottleneck in distributed learning. One of the most common mitigation strategies is to compress the information exchanged, thereby reducing communication overhead. To counteract the degradation in convergence associated with compressed communication, error feedback schemes---most notably EF and EF21---were introduced. In this work, we provide a tight analysis of both of these methods. Specifically, we find the Lyapunov function that yields the best possible convergence rate for each method---with matching lower bounds.  This principled approach yields sharp performance guarantees and enables a rigorous, apples-to-apples comparison between EF, EF21, and compressed gradient descent. Our analysis is carried out in the simplified single-agent setting, which allows for clean theoretical insights and fair comparison of the underlying mechanisms.",
      "keywords": "distributed optimization; distributed learning; error feedback; EF; EF21; tight analysis; performance estimation; convex optimization; large-scale machine learning",
      "authors": "Daniel Berg Thomsen, Adrien Taylor, Aymeric Dieuleveut",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=hlPk6Hi43e",
      "openreview_forum_url": "https://openreview.net/forum?id=hlPk6Hi43e",
      "venue_id": "NeurIPS.cc/2025/Conference"
    },
    {
      "venue": "NeurIPS",
      "year": 2025,
      "title": "Time-o1：时间序列预测需要转换标签对齐",
      "title_en": "Time-o1: Time-Series Forecasting Needs Transformed Label Alignment",
      "abstract": "训练时间序列预测模型在设计有效的学习目标方面提出了独特的挑战。现有的方法主要利用时间均方误差，这面临着两个关键挑战：（1）标签自相关，这会导致标签序列似然性的偏差；（2）任务量过多，随着预测期的增加而增加，使优化变得复杂。为了应对这些挑战，我们提出了Time-o1，这是一种用于训练时间序列预测模型的转换增强学习目标。中心思想是将标签序列转换为具有判别意义的去相关分量。然后训练模型以对齐最重要的组件，从而有效地减轻标签自相关性并减少任务量。大量实验表明，Time-o1达到了最先进的性能，并与各种预测模型兼容。代码可在以下网址获得https://github.com/Master-PLC/Time-o1.",
      "abstract_en": "Training time-series forecast models presents unique challenges in designing effective learning objectives. Existing methods predominantly utilize the temporal mean squared error, which faces two critical challenges: (1) label autocorrelation, which leads to bias from the label sequence likelihood; (2) excessive amount of tasks, which increases with the forecast horizon and complicates optimization. To address these challenges, we propose Time-o1, a transformation-augmented learning objective for training time-series forecasting models. The central idea is to transform the label sequence into decorrelated components with discriminated significance. Models are then trained to align the most significant components, thereby effectively mitigating label autocorrelation and reducing task amount. Extensive experiments demonstrate that Time-o1 achieves state-of-the-art performance and is compatible with various forecast models. Code is available at https://github.com/Master-PLC/Time-o1.",
      "keywords": "Time-Series; Label Autocorrelation; Orthogonalization",
      "authors": "Hao Wang, Licheng Pan, Zhichao Chen, Xu Chen, Qingyang Dai, Lei Wang, Haoxuan Li, Zhouchen Lin",
      "github_links": "https://github.com/Master-PLC/Time-o1",
      "pdf_download_url": "https://openreview.net/pdf?id=RxWILaXuhb",
      "openreview_forum_url": "https://openreview.net/forum?id=RxWILaXuhb",
      "venue_id": "NeurIPS.cc/2025/Conference"
    },
    {
      "venue": "NeurIPS",
      "year": 2025,
      "title": "易受攻击的数据感知对抗训练",
      "title_en": "Vulnerable Data-Aware Adversarial Training",
      "abstract": "快速对抗训练（FAT）被认为是计算密集型对抗训练的最有效替代方案之一。一般来说，FAT方法对目标任务的每个样本都同等重视。然而，每个样本与决策边界之间的距离是不同的，远离决策边界的学习样本（即对对抗鲁棒性不太重要）会带来额外的训练成本，并导致次优结果。为了解决这个问题，我们在这项研究中提出了脆弱的数据感知对抗训练（VDAT）。具体来说，我们首先提出了一种基于边际的漏洞计算方法来衡量数据样本的漏洞。此外，我们提出了一种漏洞感知数据过滤方法，以减少对抗训练的训练数据，从而提高训练效率。这些实验是在CIFAR-10、CIFAR-100和ImageNet-1K上进行的对抗训练和鲁棒神经架构搜索。结果表明，VDAT的效率比最先进的FAT方法高出76%，同时在两种情况下都提高了自然精度和对抗精度。此外，可视化和消融研究表明了VDAT中设计的两个核心组件的有效性。",
      "abstract_en": "Fast adversarial training (FAT) has been considered as one of the most effective alternatives to the computationally-intensive adversarial training. Generally, FAT methods pay equal attention to each sample of the target task. However, the distance between each sample and the decision boundary is different, learning samples which are far from the decision boundary (i.e., less important to adversarial robustness) brings additional training cost and leads to sub-optimal results. To tackle this issue, we present vulnerable data-aware adversarial training (VDAT) in this study. Specifically, we first propose a margin-based vulnerability calculation method to measure the vulnerability of data samples. Moreover, we propose a vulnerability-aware data filtering method to reduce the training data for adversarial training thus improve the training efficiency. The experiments are conducted in terms of adversarial training and robust neural architecture search on CIFAR-10, CIFAR-100, and ImageNet-1K. The results demonstrate that VDAT is up to 76% more efficient than state-of-the-art FAT methods, while achieving improvements regarding the natural accuracy and adversarial accuracy in both scenarios. Furthermore, the visualizations and ablation studies show the effectiveness of both core components designed in VDAT.",
      "keywords": "Adversarial Training; Adversarial Robustness; Decision Boundary Analysis",
      "authors": "Yuqi Feng, Jiahao Fan, Yanan Sun",
      "github_links": "",
      "pdf_download_url": "https://openreview.net/pdf?id=yrrU5YChQr",
      "openreview_forum_url": "https://openreview.net/forum?id=yrrU5YChQr",
      "venue_id": "NeurIPS.cc/2025/Conference"
    }
  ],
  "updated_at": "2025-12-19 14:55:43"
}